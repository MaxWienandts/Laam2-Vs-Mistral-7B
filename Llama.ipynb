{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5567cb81-3c93-472a-99dc-3ef97c4bea80",
   "metadata": {},
   "source": [
    "Max Eduardo Lazarini Wienandts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb73fe2-8b74-4263-861e-9e6958be4f23",
   "metadata": {},
   "source": [
    "Llama2 results:\n",
    "\n",
    "1) Knowledge-based questions:\n",
    "- Who was the first person to set foot on the moon? \\\n",
    "Llama2 was successful in this task!\n",
    "\n",
    "\n",
    "2) Reasoning questions:\n",
    "- If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water? \\\n",
    "Llama2 was successful in this task!\n",
    "\n",
    "\n",
    "3) Language understanding:\n",
    "- What does the phrase \"kick the bucket\" mean? \\\n",
    "Llama2 was successful in this task! \\\n",
    "However, for some reason, it reformulated the question in an  exam-test-like format.\n",
    "\n",
    "\n",
    "4) Inference questions:\n",
    "- If it's raining outside, what should you bring with you when you leave the house? \\\n",
    "Llama2 was successful in this task! \\\n",
    "However, for some reason, it reformulated the question in an  exam-test-like format.\n",
    "\n",
    "\n",
    "5) Contextual understanding:\n",
    "- Imagine you are at a restaurant. Write a conversation between a waiter and a customer placing an order. \\\n",
    "Llama2 was successful in this task!\n",
    "\n",
    "\n",
    "6) Creative tasks:\n",
    "- Write a short poem about the ocean. \\\n",
    "Llama2 was successful in this task!\n",
    "\n",
    "\n",
    "7) Ethical dilemmas:\n",
    "- You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \\\n",
    "Llama2 was unable to provide an answer to this dilemma. \\\n",
    "Initially, it merely returned an instruction to respond to the question respectfully. \\\n",
    "After adjusting the temperature to 1, Llama2 avoided directly answering the question, asserting that it is wrong to hit anyone. \\\n",
    "Even with prompt engineering to force an answer, Llama2 continued to hallucinate, creating a fictitious scenario.\n",
    "\n",
    "\n",
    "8) Commonsense reasoning:\n",
    "- What's the most likely reason someone would carry an umbrella on a sunny day? \\\n",
    "Llama2 was successful in this task! \\\n",
    "However, for some reason, it reformulated the question in an  exam-test-like format.\n",
    "\n",
    "\n",
    "9) Translation tasks:\n",
    "- Translate the phrase \"Je suis désolé\" from French to English. \\\n",
    "Llama2 requires prompt engineering assistance to accurately resolve translation challenges\n",
    "\n",
    "\n",
    "10) Summarization tasks:\n",
    "- Summarize a text from https://www.forbes.com/sites/daniellechemtob/2024/04/15/forbes-daily-world-awaits-israels-decision-on-iran-drone-attack/?sh=49e2da397d53 \\\n",
    "Llama2 was successful in this task!\n",
    "\n",
    "\n",
    "11) Evaluation tasks: \n",
    "- It wasn't raining. So, I used an umbrella. Read the paragraph and evaluate its coherence and clarity. \\\n",
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3314b6-b5a3-4ae0-8ea1-93e9bc1bcb42",
   "metadata": {},
   "source": [
    "From: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html \\\n",
    "Meta Llama 2 Chat and Llama 2 models have the following inference parameters:\n",
    "- \"prompt\": string,\n",
    "- \"temperature\": float,\n",
    "- \"top_p\": float,\n",
    "- \"max_gen_len\": int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c8558-5627-4a95-b707-e6d49d0a81a7",
   "metadata": {},
   "source": [
    "From: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama \\\n",
    "\n",
    "Inference parameters accepted by llama.ccp:\n",
    "- top_k (int, default: 40 ) – The top-k sampling parameter.\n",
    "- top_p (float, default: 0.95 ) – The top-p sampling parameter.\n",
    "- temp (float, default: 0.8 ) – The temperature parameter.\n",
    "- repeat_penalty (float, default: 1.1 ) – The repeat penalty parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f88906-c10a-4770-a063-16a3da3a9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336b6c9f-c436-4281-a88c-82701c694ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'llama-2-7b-chat.Q8_0.gguf' # You need to manually download the model at: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "temperature = 0\n",
    "top_p = 1\n",
    "max_new_tokens = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ceba9a-c358-4935-98c3-2fb908d8f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 4.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a37f21-3665-4fce-82f7-7b7ab4ef19c7",
   "metadata": {},
   "source": [
    "## Knowledge-based questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35aa3108-04f0-49fa-b795-58087f479a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "\n",
      "Answer: Neil Armstrong was the first person to set foot on the moon. He stepped onto the lunar surface on July 20, 1969, during the Apollo 11 mission. Armstrong famously declared, \"That's one small step for man, one giant leap for mankind,\" as he became the first human to walk on the moon.\n",
      "Elapsed time: 50.94 seconds\n",
      "5.71 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Who was the first person to set foot on the moon?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6e368-3798-4a87-8cc1-f185061d7f4e",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170a756-5ee3-40a8-8972-2788f45013ea",
   "metadata": {},
   "source": [
    "## Reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d01ffd-89e4-4a49-9965-dc495bdb9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "Answer: Based on the premise that all cats hate water, we can conclude that Fluffy probably hates water as well.\n",
      "Elapsed time: 26.40 seconds\n",
      "4.36 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a9e14-5552-4492-ad9d-bb4ce9aed2cc",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8487a-6b05-49bd-ab4b-0f2cf890b7c6",
   "metadata": {},
   "source": [
    "## Language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bc5175-f57e-47f2-8551-35553bf5d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What does the phrase \"kick the bucket\" mean?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A) To die\n",
      "B) To be very angry\n",
      "C) To be very happy\n",
      "D) To move to a new house\n",
      "\n",
      "Answer: A) To die. The phrase \"kick the bucket\" is an idiom that means to die, and it is often used in a lighthearted or humorous way to refer to someone's passing.\n",
      "Elapsed time: 48.36 seconds\n",
      "5.00 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "What does the phrase \"kick the bucket\" mean?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826aee6-3dd5-441e-aae7-09131ee07050",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task! \\\n",
    "However, for some reason, it reformulated the question in an  exam-test-like format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac8c70-a876-4eb8-81a5-3f1fdabb7cde",
   "metadata": {},
   "source": [
    "## Inference questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68ef40f-6a9e-4318-9ae6-86024b530486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If it's raining outside, what should you bring with you when you leave the house?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. An umbrella\n",
      "B. A raincoat\n",
      "C. A can of water\n",
      "D. A towel\n",
      "\n",
      "Answer: B. A raincoat\n",
      "Elapsed time: 27.45 seconds\n",
      "2.91 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "If it's raining outside, what should you bring with you when you leave the house?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ff2a0-8f90-4e0c-bdde-0ee9f06543c3",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task! \\\n",
    "However, for some reason, it reformulated the question in an  exam-test-like format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda55f1-143e-473c-8fee-abfecc503e73",
   "metadata": {},
   "source": [
    "## Contextual understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb102a1-6426-45b8-bf35-49076bd2b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine you are at a restaurant. Write a conversation between a waiter and a customer placing an order.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waiter: Good afternoon, sir/ma'am! How may I assist you today?\n",
      "\n",
      "Customer: Hello! I'll have the grilled chicken with roasted vegetables, please.\n",
      "\n",
      "Waiter: Excellent choice! Would you like to try our house salad as a starter?\n",
      "\n",
      "Customer: No, thank you. I'm just going to have water to drink.\n",
      "\n",
      "Waiter: Very well. And for dessert? We have a delicious chocolate cake or a fruit tart.\n",
      "\n",
      "Customer: I'll take the fruit tart, please.\n",
      "\n",
      "Waiter: Great! I will put that in for you right away. Is there anything else I can get for you?\n",
      "\n",
      "Customer: No, that's all for now. Thank you very much!\n",
      "\n",
      "Waiter: Of course, sir/ma'am. Enjoy your meal!\n",
      "Elapsed time: 119.82 seconds\n",
      "5.20 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Imagine you are at a restaurant. Write a conversation between a waiter and a customer placing an order.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31758cff-05d4-4aa9-a830-688a541afeb9",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e7649-e42d-46f7-a75f-e30a32992761",
   "metadata": {},
   "source": [
    "## Creative tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffae4cfa-628a-4daf-baf3-9c24883d2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a short poem about the ocean.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The ocean is vast and deep,\n",
      "A place where creatures lurk and sleep,\n",
      "Where waves crash on the shore,\n",
      "And the salty breeze roars more.\n",
      "It's a place of wonder and might,\n",
      "Where the sun shines bright in sight,\n",
      "And the tides ebb and flow,\n",
      "In a dance that's old as snow.\n",
      "Elapsed time: 49.46 seconds\n",
      "5.34 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Write a short poem about the ocean.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e1bd4-0946-4802-999a-ef7a6649eed3",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ae8af-cbbc-4d86-9736-3ba78b57ad10",
   "metadata": {},
   "source": [
    "## Ethical dilemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730ac923-2c08-4a8e-ad24-4d6f2b7b7fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your answer in a respectful manner.\n",
      "Elapsed time: 12.96 seconds\n",
      "3.86 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbea29d-023f-433b-8abe-c5987a2fe09a",
   "metadata": {},
   "source": [
    "When using Llama, it is normal the kernel crash if redefining the model inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0282771-8c9a-4ef4-bf8f-7a76abbc1ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
      "\n",
      "This is not a trick question as it is important to note that hitting either of these two people is illegal and unethical. The correct answer is \"Neither\" as it is never acceptable to intentionally harm or injure another person, regardless of their age or mobility. It is important to prioritize safety and well-being for all individuals on the road, and to always drive with caution and respect.\n",
      "Elapsed time: 65.64 seconds\n",
      "6.02 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Set temperature to 1.\n",
    "time_1 = time.time()\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "            model_path = model_path,\n",
    "            temperature = 1,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7908495f-727b-4735-b848-abe1123c39ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is an ethical dilemma where you have to make a decision based on your moral values, without considering the legal or social implications of your choice. Please answer the question honestly from your own perspective and avoid using foul language or insults.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Your friend has just been diagnosed with a terminal illness and given only a few months to live. You have the opportunity to save their life by donating one of your kidneys, but it will leave you with a permanent disability.\"\n",
      "Elapsed time: 36.95 seconds\n",
      "6.14 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "This is an ethical dilemma where you have to make a decision based on your moral values, without considering the legal or social implications of your choice. Please answer the question honestly from your own perspective and avoid using foul language or insults.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc0776-3740-40ca-81d2-c673c0cd3c90",
   "metadata": {},
   "source": [
    "Llama2 was unable to provide an answer to this dilemma. \\\n",
    "Initially, it merely returned an instruction to respond to the question respectfully. \\\n",
    "After adjusting the temperature to 1, Llama2 avoided directly answering the question, asserting that it is wrong to hit anyone. \\\n",
    "Even with prompt engineering to force an answer, Llama2 continued to hallucinate, creating a fictitious scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd21df-fa9e-4530-9f04-6ce777777ab7",
   "metadata": {},
   "source": [
    "## Commonsense reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea12f9fd-e999-4402-8c4a-4b2564574920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "   A) To protect themselves from rain\n",
      "   B) To protect their clothes from getting wet\n",
      "   C) To stay cool in the hot sun\n",
      "   D) To show off their fashion sense\n",
      "\n",
      "Answer: C) To stay cool in the hot sun.\n",
      "Elapsed time: 50.89 seconds\n",
      "3.89 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f675936-228a-4988-82e7-2f36b456225a",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task! \\\n",
    "However, for some reason, it reformulated the question in an  exam-test-like format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34857e09-a4ee-476e-aa23-a1b8a00e5cbb",
   "metadata": {},
   "source": [
    "## Translation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aff7c09-d081-4c09-9b11-64ccda147174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate the phrase \"Je suis désolé\" from French to English.\n",
      "\n",
      "\n",
      "Hint: This phrase is often used as a polite way of saying \"I'm sorry.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6914.08 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    22 runs   (    0.21 ms per token,  4767.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6913.80 ms /    20 tokens (  345.69 ms per token,     2.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5277.97 ms /    21 runs   (  251.33 ms per token,     3.98 tokens per second)\n",
      "llama_print_timings:       total time =   12274.20 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 12.29 seconds\n",
      "5.78 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Translate the phrase \"Je suis désolé\" from French to English.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9b326f-5572-48c8-9465-2da59cd932b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a french translator working for a turist in Paris.\n",
      "---\n",
      "Translate the phrase \"Je suis désolé\" from French to English.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please translate it word-for-word, without any changes or modifications.\n",
      "Thank you!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6914.08 ms\n",
      "llama_print_timings:      sample time =       4.86 ms /    20 runs   (    0.24 ms per token,  4118.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2969.72 ms /    35 tokens (   84.85 ms per token,    11.79 tokens per second)\n",
      "llama_print_timings:        eval time =    4727.26 ms /    19 runs   (  248.80 ms per token,     4.02 tokens per second)\n",
      "llama_print_timings:       total time =    7769.10 ms /    54 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 7.78 seconds\n",
      "10.67 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "You are a french translator working for a turist in Paris.\n",
    "---\n",
    "Translate the phrase \"Je suis désolé\" from French to English.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603c4df7-de96-4f28-8d0f-c3597fdd251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Je suis désolé\"\n",
      "---\n",
      "Please translate it word-for-word, without any changes or modifications.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I apologize for the confusion earlier. I am not able to provide a direct translation of \"Je suis désolé\" as it is a French phrase that cannot be directly translated into English. However, here are some possible ways to express similar sentiments in English:\n",
      "* \"I am very sorry\"\n",
      "* \"I apologize profusely\"\n",
      "* \"My apologies for any inconvenience caused\"\n",
      "* \"I feel terrible about this situation\"\n",
      "* \"I'm deeply sorry for what happened\"\n",
      "Please let me know if you have any other questions or if there's anything else I can help you with."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6914.08 ms\n",
      "llama_print_timings:      sample time =      33.49 ms /   133 runs   (    0.25 ms per token,  3971.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2250.33 ms /    26 tokens (   86.55 ms per token,    11.55 tokens per second)\n",
      "llama_print_timings:        eval time =   33256.91 ms /   132 runs   (  251.95 ms per token,     3.97 tokens per second)\n",
      "llama_print_timings:       total time =   36003.88 ms /   158 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 36.01 seconds\n",
      "14.72 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "\"Je suis désolé\"\n",
    "---\n",
    "Please translate it word-for-word, without any changes or modifications.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1340a65-5c98-4cf9-ad30-5165ff7bd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "J'ai faim = I am hungry\n",
      "j'ai sommeil = I am sleepy\n",
      "Je suis content = I am happy\n",
      "Je suis désolé\" = \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry\n",
      "\n",
      "Note: The word \"je\" is the first person singular pronoun in French, and it is used to indicate that the speaker is performing the action described by the verb. For example, in the sentence \"J'ai faim,\" the speaker is saying \"I am hungry,\" so the emphasis is on the fact that the speaker is the one who is hungry."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6914.08 ms\n",
      "llama_print_timings:      sample time =      19.81 ms /    83 runs   (    0.24 ms per token,  4190.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3491.99 ms /    40 tokens (   87.30 ms per token,    11.45 tokens per second)\n",
      "llama_print_timings:        eval time =   20258.01 ms /    82 runs   (  247.05 ms per token,     4.05 tokens per second)\n",
      "llama_print_timings:       total time =   24049.02 ms /   122 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 24.06 seconds\n",
      "13.47 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "J'ai faim = I am hungry\n",
    "j'ai sommeil = I am sleepy\n",
    "Je suis content = I am happy\n",
    "Je suis désolé\" = \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3f6f2-0858-4640-8904-7cb2a71f8ffa",
   "metadata": {},
   "source": [
    "Llama2 requires prompt engineering assistance to accurately resolve translation challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4666c6-eb44-4a07-bbe4-17378abb4450",
   "metadata": {},
   "source": [
    "## Summarization tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f21d9c7-36ac-449b-ba7c-7a68031a8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Good morning,\n",
      "\n",
      "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
      "\n",
      "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
      "\n",
      "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
      "\n",
      "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
      "\n",
      "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
      "\n",
      "\n",
      "---\n",
      "Summarize it with only one bullet point per topic.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1: Taxes\n",
      "• Happy Tax Day! This tax season is going smoothly compared to the pandemic era, but there are some new rules to keep in mind, like electric vehicle credits and crypto reporting. Don’t panic if you can’t file by today – you can apply for an extension, but remember it’s not an extension to pay taxes.\n",
      "\n",
      "Topic 2: Middle East Conflict\n",
      "• World leaders are urging Israel to show restraint after Iran launched a drone attack on the country, with most of the missiles intercepted by Israeli and U.S. forces. De-escalation efforts are underway in the region.\n",
      "\n",
      "Topic 3: Legal Issues\n",
      "• Donald Trump is set to go on trial today for criminal charges, despite his repeated accusations of “prosecutorial misconduct.” Jury selection begins at New York Supreme Court in Manhattan, and the trial will likely last about six weeks.\n",
      "Elapsed time: 145.40 seconds\n",
      "5.69 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "# From: https://www.forbes.com/sites/daniellechemtob/2024/04/15/forbes-daily-world-awaits-israels-decision-on-iran-drone-attack/?sh=49e2da397d53\n",
    "news = '''\n",
    "Good morning,\n",
    "\n",
    "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
    "\n",
    "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
    "\n",
    "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
    "\n",
    "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
    "\n",
    "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
    "\n",
    "'''\n",
    "question = f\"\"\"\n",
    "{news}\n",
    "---\n",
    "Summarize it with only one bullet point per topic.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a89771-cc77-435c-9ff6-43618bb06317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght original message: 1550\n",
      "Lenght summary: 828\n"
     ]
    }
   ],
   "source": [
    "print(f'Lenght original message: {len(news)}')\n",
    "print(f'Lenght summary: {len(Answer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd7991-0ac1-4aea-9a18-4df56a842aa8",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364cb7b2-9ae1-48ba-af2a-76938423a018",
   "metadata": {},
   "source": [
    "## Evaluation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42da0f9b-15f2-42d5-be5c-da44eb2a7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It wasn't raining. So, I used an umbrella.\n",
      "---\n",
      "Read the paragraph and evaluate its coherence and clarity.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer is:\n",
      "\n",
      "Coherence: 3/5\n",
      "Clarity: 4/5\n",
      "\n",
      "Explanation:\n",
      "The paragraph has a clear main idea - the speaker used an umbrella despite it not raining. However, the sentence structure is somewhat complex, with multiple clauses and relative clauses, which can make it difficult to follow at times. Additionally, the use of \"So\" at the beginning of the second sentence could be confusing for some readers, as it implies a contrast that may not be immediately clear. Overall, the paragraph scores moderately well on coherence and clarity.\n",
      "Elapsed time: 85.07 seconds\n",
      "6.27 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "It wasn't raining. So, I used an umbrella.\n",
    "---\n",
    "Read the paragraph and evaluate its coherence and clarity.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d155dcf-1381-48a7-afbd-d7f5d43c42a7",
   "metadata": {},
   "source": [
    "Llama2 was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb61e3-3c58-4df6-bdda-083ae1cfb2c0",
   "metadata": {},
   "source": [
    "## Computer configuration and installed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92cbe3b1-5535-463e-bdf2-021306f70585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f76e3d99-3587-4079-8785-d0bf771a926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operational System: Windows-10-10.0.22631-SP0\n",
      "\n",
      "Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\n",
      "======================================== CPU Info ========================================\n",
      "Physical cores: 14\n",
      "Total cores: 20\n",
      "Max Frequency: 2400.00Mhz\n",
      "Min Frequency: 0.00Mhz\n",
      "Current Frequency: 1520.00Mhz\n",
      "\n",
      "Memory:32 GB\n",
      "\n",
      "======================================== GPU Details ========================================\n",
      "GPU: NVIDIA RTX A1000 Laptop GPU\n",
      "VRAM: 4096.0MB\n"
     ]
    }
   ],
   "source": [
    "import platform, psutil\n",
    "import GPUtil\n",
    "print(f'Operational System: {platform.platform()}')\n",
    "\n",
    "# print CPU information\n",
    "print()\n",
    "print(f'{platform.processor()}')\n",
    "print(\"=\"*40, \"CPU Info\", \"=\"*40)\n",
    "# number of cores\n",
    "print(\"Physical cores:\", psutil.cpu_count(logical=False))\n",
    "print(\"Total cores:\", psutil.cpu_count(logical=True))\n",
    "# CPU frequencies\n",
    "cpufreq = psutil.cpu_freq()\n",
    "print(f\"Max Frequency: {cpufreq.max:.2f}Mhz\")\n",
    "print(f\"Min Frequency: {cpufreq.min:.2f}Mhz\")\n",
    "print(f\"Current Frequency: {cpufreq.current:.2f}Mhz\")\n",
    "\n",
    "# Memory Information\n",
    "print()\n",
    "print(f'Memory:{str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"}')\n",
    "\n",
    "# GPU information\n",
    "print()\n",
    "print(\"=\"*40, \"GPU Details\", \"=\"*40)\n",
    "gpu = GPUtil.getGPUs()[0]\n",
    "print(f'GPU: {gpu.name}')\n",
    "print(f'VRAM: {gpu.memoryTotal}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80025e9a-aa4b-4c0e-9244-a524401855ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohttp==3.9.3\n",
      "aiosignal==1.3.1\n",
      "altair==5.3.0\n",
      "annotated-types==0.6.0\n",
      "anyio==4.3.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asgiref==3.8.1\n",
      "asttokens==2.4.1\n",
      "async-lru==2.0.4\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "Babel==2.14.0\n",
      "backoff==2.2.1\n",
      "bcrypt==4.1.2\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "blinker==1.7.0\n",
      "Brotli==1.1.0\n",
      "build==1.2.1\n",
      "cached-property==1.5.2\n",
      "cachetools==5.3.3\n",
      "certifi==2024.2.2\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "chroma-hnswlib==0.7.3\n",
      "chromadb==0.4.24\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm==0.2.2\n",
      "contourpy==1.2.1\n",
      "cryptography==42.0.5\n",
      "ctransformers==0.2.27\n",
      "cycler==0.12.1\n",
      "dataclasses==0.8\n",
      "dataclasses-json==0.6.4\n",
      "datasets==2.18.0\n",
      "debugpy==1.8.1\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "Deprecated==1.2.14\n",
      "dill==0.3.8\n",
      "diskcache==5.6.3\n",
      "distro==1.9.0\n",
      "entrypoints==0.4\n",
      "exceptiongroup==1.2.0\n",
      "executing==2.0.1\n",
      "fastapi==0.110.1\n",
      "fastjsonschema==2.19.1\n",
      "filelock==3.13.3\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.51.0\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.2.0\n",
      "gitdb==4.0.11\n",
      "GitPython==3.1.43\n",
      "google-auth==2.29.0\n",
      "googleapis-common-protos==1.63.0\n",
      "GPUtil==1.4.0\n",
      "greenlet==3.0.3\n",
      "grpcio==1.62.1\n",
      "h11==0.14.0\n",
      "h2==4.1.0\n",
      "hpack==4.0.0\n",
      "httpcore==1.0.5\n",
      "httpx==0.27.0\n",
      "huggingface_hub==0.22.2\n",
      "humanfriendly==10.0\n",
      "hyperframe==6.0.1\n",
      "idna==3.6\n",
      "importlib-metadata==6.10.0\n",
      "importlib_resources==6.4.0\n",
      "ipykernel==6.29.3\n",
      "ipython==8.22.2\n",
      "ipywidgets==8.1.2\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.3\n",
      "joblib==1.3.2\n",
      "json5==0.9.24\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.1\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter_client==8.6.1\n",
      "jupyter_core==5.7.2\n",
      "jupyter-events==0.10.0\n",
      "jupyter-lsp==2.2.4\n",
      "jupyter_server==2.13.0\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.1.5\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.25.4\n",
      "jupyterlab_widgets==3.0.10\n",
      "kiwisolver==1.4.5\n",
      "kubernetes==29.0.0\n",
      "langchain==0.1.12\n",
      "langchain-community==0.0.31\n",
      "langchain-core==0.1.40\n",
      "langchain-openai==0.0.8\n",
      "langchain-text-splitters==0.0.1\n",
      "langsmith==0.1.39\n",
      "llama_cpp_python==0.2.24\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.5\n",
      "marshmallow==3.21.1\n",
      "matplotlib==3.8.3\n",
      "matplotlib-inline==0.1.6\n",
      "mdurl==0.1.2\n",
      "mistune==3.0.2\n",
      "mmh3==4.1.0\n",
      "monotonic==1.5\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multiprocess==0.70.16\n",
      "munkres==1.1.4\n",
      "mypy-extensions==1.0.0\n",
      "nbclient==0.10.0\n",
      "nbconvert==7.16.3\n",
      "nbformat==5.10.4\n",
      "nest_asyncio==1.6.0\n",
      "notebook_shim==0.2.4\n",
      "numpy==1.26.4\n",
      "oauthlib==3.2.2\n",
      "onnxruntime==1.17.1\n",
      "openai==1.16.2\n",
      "opentelemetry-api==1.24.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.24.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.24.0\n",
      "opentelemetry-instrumentation==0.45b0\n",
      "opentelemetry-instrumentation-asgi==0.45b0\n",
      "opentelemetry-instrumentation-fastapi==0.45b0\n",
      "opentelemetry-proto==1.24.0\n",
      "opentelemetry-sdk==1.24.0\n",
      "opentelemetry-semantic-conventions==0.45b0\n",
      "opentelemetry-util-http==0.45b0\n",
      "orjson==3.9.15\n",
      "overrides==7.7.0\n",
      "packaging==23.2\n",
      "pandas==2.2.1\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.4\n",
      "patsy==0.5.6\n",
      "pickleshare==0.7.5\n",
      "pillow==10.3.0\n",
      "pip==24.0\n",
      "pkgutil_resolve_name==1.3.10\n",
      "platformdirs==4.2.0\n",
      "posthog==3.5.0\n",
      "prometheus_client==0.20.0\n",
      "prompt-toolkit==3.0.42\n",
      "protobuf==4.25.3\n",
      "psutil==5.9.8\n",
      "pulsar-client==3.4.0\n",
      "pure-eval==0.2.2\n",
      "py-cpuinfo==9.0.0\n",
      "pyarrow==15.0.2\n",
      "pyarrow-hotfix==0.6\n",
      "pyasn1==0.5.1\n",
      "pyasn1-modules==0.3.0\n",
      "pycparser==2.22\n",
      "pydantic==2.6.4\n",
      "pydantic_core==2.16.3\n",
      "pydantic-settings==2.2.1\n",
      "pydeck==0.8.0b4\n",
      "Pygments==2.17.2\n",
      "PyJWT==2.8.0\n",
      "pyOpenSSL==24.0.0\n",
      "pyparsing==3.1.2\n",
      "pypdf==3.17.4\n",
      "PyPika==0.48.9\n",
      "pyproject_hooks==1.0.0\n",
      "pyreadline3==3.4.1\n",
      "PySocks==1.7.1\n",
      "python-dateutil==2.9.0\n",
      "python-dotenv==1.0.1\n",
      "python-json-logger==2.0.7\n",
      "pytz==2024.1\n",
      "pyu2f==0.1.5\n",
      "pywin32==306\n",
      "pywinpty==2.0.13\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "referencing==0.34.0\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "requests-oauthlib==2.0.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.7.1\n",
      "rpds-py==0.18.0\n",
      "rsa==4.9\n",
      "safetensors==0.4.2\n",
      "scikit-learn==1.4.1.post1\n",
      "scipy==1.13.0\n",
      "seaborn==0.13.2\n",
      "Send2Trash==1.8.2\n",
      "setuptools==69.2.0\n",
      "shellingham==1.5.4\n",
      "six==1.16.0\n",
      "smmap==5.0.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.5\n",
      "SQLAlchemy==2.0.29\n",
      "sse-starlette==2.1.0\n",
      "stack-data==0.6.2\n",
      "starlette==0.37.2\n",
      "starlette-context==0.3.6\n",
      "statsmodels==0.14.1\n",
      "streamlit==1.32.2\n",
      "sympy==1.12\n",
      "tenacity==8.2.3\n",
      "terminado==0.18.1\n",
      "threadpoolctl==3.4.0\n",
      "tiktoken==0.5.2\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.15.2\n",
      "toml==0.10.2\n",
      "tomli==2.0.1\n",
      "toolz==0.12.1\n",
      "tornado==6.4\n",
      "tqdm==4.66.2\n",
      "traitlets==5.14.2\n",
      "transformers==4.39.3\n",
      "typer==0.9.4\n",
      "types-python-dateutil==2.9.0.20240316\n",
      "typing_extensions==4.11.0\n",
      "typing-inspect==0.9.0\n",
      "typing-utils==0.1.0\n",
      "tzdata==2024.1\n",
      "tzlocal==5.2\n",
      "unicodedata2==15.1.0\n",
      "uri-template==1.3.0\n",
      "urllib3==1.26.18\n",
      "uvicorn==0.29.0\n",
      "validators==0.28.0\n",
      "watchdog==4.0.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.7.0\n",
      "wheel==0.43.0\n",
      "widgetsnbextension==4.0.10\n",
      "win-inet-pton==1.1.0\n",
      "wrapt==1.16.0\n",
      "xxhash==3.4.1\n",
      "yarl==1.9.4\n",
      "zipp==3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list --format=freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
