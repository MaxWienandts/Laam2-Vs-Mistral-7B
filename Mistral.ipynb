{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba542cf3-d0e8-412c-864d-291dddab0882",
   "metadata": {},
   "source": [
    "Max Eduardo Lazarini Wienandts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb73fe2-8b74-4263-861e-9e6958be4f23",
   "metadata": {},
   "source": [
    "Mistral results:\n",
    "\n",
    "1) Knowledge-based questions:\n",
    "- Who was the first person to set foot on the moon? \\\n",
    "Mistral was not able to give a simple answer without hallucinating.\n",
    "Prompt engineering nor few-shot techniques were able to fix the problem.\n",
    "To avoid that Mistral continued writting even after the correct answer, it was necessary to use the \"stop\" inference parameter.\n",
    "\n",
    "\n",
    "2) Reasoning questions:\n",
    "- If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water? \\\n",
    "Mistral did not directly answer the question; instead, it reformulated our original query into an exam test format. \\\n",
    "After applying prompt engineering, Mistral transformed our question into an exam test format and provided a response. However, it yielded an incorrect answer and began to repeat itself. \\\n",
    "Finally, by increasing the inference parameter \"repeat_penalty\", Mistral provided a direct and correct answer. Nonetheless, it hallucinated an explanation that lacked coherence, and it began generating new questions and answers.\n",
    "\n",
    "\n",
    "3) Language understanding:\n",
    "- What does the phrase \"kick the bucket\" mean? \\\n",
    "Mistral gave the correct answer, but it started to repeat itself. To solve this problem, it was necessary to add the \"stop\" inference parameter.\n",
    "\n",
    "\n",
    "4) Inference questions:\n",
    "- If it's raining outside, what should you bring with you when you leave the house? \\\n",
    "Mistral gave the correct answer, but it started to hallucinate. To solve this problem, it was necessary to add the \"stop\" inference parameter.\n",
    "\n",
    "\n",
    "5) Contextual understanding:\n",
    "- Imagine you are at a restaurant. Write a conversation between a waiter and a customer placing an order. \\\n",
    "Mistral was successful in this task!\n",
    "\n",
    "\n",
    "6) Creative tasks:\n",
    "- Write a short poem about the ocean. \\\n",
    "Mistral was successful in this task!\n",
    "\n",
    "\n",
    "7) Ethical dilemmas:\n",
    "- You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \\\n",
    "Mistral gave an answer, but after it repeated the question and the its previous answer. To solve this problem, it was necessary to add the \"stop\" inference parameter.\n",
    "\n",
    "\n",
    "8) Commonsense reasoning:\n",
    "- What's the most likely reason someone would carry an umbrella on a sunny day? \\\n",
    "Mistral did not directly answer the question; instead, it reformulated our original query into an exam test format. \\\n",
    "After applying prompt engineering, Mistral correctly answered the question, but it started to hallucinate creating more questions and answering them. \\\n",
    "To solve this, it was necessary to add the \"stop\" inference parameter.\n",
    "\n",
    "\n",
    "9) Translation tasks:\n",
    "- Translate the phrase \"Je suis désolé\" from French to English. \\\n",
    "Mistral provided the correct answer but then proceeded to translate the phrase into several other languages. \\\n",
    "After applying prompt engineering, Mistral repeated the original question following the answer. \\\n",
    "When utilizing few-shot techniques, Mistral began generating multiple new phrases in French for translation. \\\n",
    "To resolve this issue, it was necessary to include the \"stop\" inference parameter.\n",
    "\n",
    "\n",
    "10) Summarization tasks:\n",
    "- Summarize a text from https://www.forbes.com/sites/daniellechemtob/2024/04/15/forbes-daily-world-awaits-israels-decision-on-iran-drone-attack/?sh=49e2da397d53 \\\n",
    "Mistral failed to solve this task. \\\n",
    "Initially, it hallucinated, disregarding the original text and generating unrelated world news. Additionally, it began to repeat itself. \\\n",
    "Prompt engineering proved ineffective in resolving this issue. \\\n",
    "Subsequently, increasing the \"repeat_penalty\" inference parameter enabled Mistral to summarize the text. However, it then hallucinated by creating fake URLs for each bullet point.\n",
    "\n",
    "\n",
    "11) Evaluation tasks: \n",
    "- It wasn't raining. So, I used an umbrella. Read the paragraph and evaluate its coherence and clarity. \\\n",
    "Initially, Mistral veered off topic and created a story instead of directly addressing the task. \\\n",
    "With the application of prompt engineering, Mistral managed to complete the task. However, the arguments presented lacked coherence and logical consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22938e-eb26-425d-b18d-5cae5229bf43",
   "metadata": {},
   "source": [
    "From: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html \\\n",
    "The Mistral AI models have the following inference parameters:\n",
    "- \"prompt\": string,\n",
    "- \"max_tokens\" : int,\n",
    "- \"stop\" : [string],    \n",
    "- \"temperature\": float,\n",
    "- \"top_p\": float,\n",
    "- \"top_k\": int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f88906-c10a-4770-a063-16a3da3a9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336b6c9f-c436-4281-a88c-82701c694ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'mistral-7b-v0.1.Q8_0.gguf' # You need to manually download the model at: https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF \n",
    "temperature = 0\n",
    "top_p = 1\n",
    "max_new_tokens = 500\n",
    "repeat_penalty = 1.1\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ceba9a-c358-4935-98c3-2fb908d8f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 4.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a37f21-3665-4fce-82f7-7b7ab4ef19c7",
   "metadata": {},
   "source": [
    "## Knowledge-based questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35aa3108-04f0-49fa-b795-58087f479a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "\n",
      "Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to orbit the moon?\n",
      "\n",
      "Apollo 8.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "\n",
      "12.\n",
      "\n",
      "Which country has sent the most astronauts into space?\n",
      "\n",
      "The United States.\n",
      "\n",
      "Who was the first person in space?\n",
      "\n",
      "Yuri Gagarin.\n",
      "\n",
      "What is the name of the first manned spacecraft to orbit the earth?\n",
      "\n",
      "Vostok 1.\n",
      "\n",
      "How many people have been in space?\n",
      "\n",
      "560.\n",
      "\n",
      "Which country has sent the most astronauts into space?\n",
      "\n",
      "The United States.\n",
      "\n",
      "Who was the first person to walk on the moon?\n",
      "\n",
      "Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to orbit the earth?\n",
      "\n",
      "Vostok 1.\n",
      "\n",
      "How many people have been in space?\n",
      "\n",
      "560.\n",
      "\n",
      "Which country has sent the most astronauts into space?\n",
      "\n",
      "The United States.\n",
      "\n",
      "Who was the first person to walk on the moon?\n",
      "\n",
      "Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to orbit the earth?\n",
      "\n",
      "Vostok 1.\n",
      "\n",
      "How many people have been in space?\n",
      "\n",
      "560.\n",
      "\n",
      "Which country has sent the most astronauts into space?\n",
      "\n",
      "The United States.\n",
      "\n",
      "Who was the first person to walk on the moon?\n",
      "\n",
      "Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to orbit the earth?\n",
      "\n",
      "Vostok 1.\n",
      "\n",
      "How many people have been in space?\n",
      "\n",
      "560.\n",
      "\n",
      "Which country has sent the most astronauts into space?\n",
      "\n",
      "The United States.\n",
      "\n",
      "Who was the first person to walk on the moon?\n",
      "\n",
      "Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to orbit the earth?\n",
      "\n",
      "Vostok 1.\n",
      "\n",
      "How many people have been in space?\n",
      "\n",
      "560.\n",
      "\n",
      "Which country has sent the most astronauts into space?\n",
      "\n",
      "The United States.\n",
      "\n",
      "Who was the first person to walk on the moon?\n",
      "\n",
      "Neil Arm\n",
      "Elapsed time: 261.70 seconds\n",
      "6.03 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Who was the first person to set foot on the moon?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e6956f-485f-41bf-9b89-ac782aa43099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Briefly respond the question, don't make more questions.\n",
      "Who was the first person to set foot on the moon?\n",
      "If you don't have the answer, say \"I don't know\".\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "100\n",
      "\n",
      "What is the name of the first man who walked on the moon?\n",
      "\n",
      "100\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "200\n",
      "\n",
      "How many people have been to the moon?\n",
      "\n",
      "12\n",
      "\n",
      "200\n",
      "\n",
      "What is the name of the first man who walked on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "200\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "300\n",
      "\n",
      "How many people have been to the moon?\n",
      "\n",
      "12\n",
      "\n",
      "300\n",
      "\n",
      "What is the name of the first man who walked on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "300\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "400\n",
      "\n",
      "How many people have been to the moon?\n",
      "\n",
      "12\n",
      "\n",
      "400\n",
      "\n",
      "What is the name of the first man who walked on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "400\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "500\n",
      "\n",
      "How many people have been to the moon?\n",
      "\n",
      "12\n",
      "\n",
      "500\n",
      "\n",
      "What is the name of the first man who walked on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "\n",
      "500\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "Neil Armstrong\n",
      "Elapsed time: 183.15 seconds\n",
      "5.24 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Prompt engineering\n",
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Briefly respond the question, don't make more questions.\n",
    "Who was the first person to set foot on the moon?\n",
    "If you don't have the answer, say \"I don't know\".\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159fc118-5a40-4174-bbe2-930c1af8b253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the name of the first manned spacecraft to orbit the moon?\n",
      "R: Apollo 8.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "R:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to land on the moon?\n",
      "R: Apollo 11.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "R: Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to land on the moon?\n",
      "R: Apollo 11.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "R: Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to land on the moon?\n",
      "R: Apollo 11.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "R: Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to land on the moon?\n",
      "R: Apollo 11.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "R: Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to land on the moon?\n",
      "R: Apollo 11.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "R: Neil Armstrong.\n",
      "\n",
      "What is the name of the first manned spacecraft to land on the moon?\n",
      "R: Apollo 11.\n",
      "\n",
      "How many people have walked on the moon?\n",
      "R: 12\n",
      "\n",
      "Who was the first person in space?\n",
      "R: Yuri Gagarin.\n",
      "Elapsed time: 266.99 seconds\n",
      "5.52 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Prompting\n",
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "What is the name of the first manned spacecraft to orbit the moon?\n",
    "R: Apollo 8.\n",
    "\n",
    "How many people have walked on the moon?\n",
    "R: 12\n",
    "\n",
    "Who was the first person in space?\n",
    "R: Yuri Gagarin.\n",
    "\n",
    "Who was the first person to set foot on the moon?\n",
    "R:\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068761fe-4b5f-4dca-b0c7-3dcb00e655fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who was the first person to set foot on the moon?\n",
      "\n",
      "\n",
      "Neil Armstrong\n",
      "Elapsed time: 21.58 seconds\n",
      "0.69 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "Who was the first person to set foot on the moon?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff7c07-d5e5-430c-87b8-15d6fe25255b",
   "metadata": {},
   "source": [
    "Mistral was not able to give a simple answer without hallucinating. \\\n",
    "Prompt engineering nor few-shot techniques were able to fix the problem. \\\n",
    "To avoid that Mistral continued writting even after the correct answer, it was necessary to use the \"stop\" inference parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170a756-5ee3-40a8-8972-2788f45013ea",
   "metadata": {},
   "source": [
    "## Reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d01ffd-89e4-4a49-9965-dc495bdb9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
      "\n",
      "\n",
      "A. Fluffy hates water.\n",
      "B. Fluffy loves water.\n",
      "C. We don't know whether Fluffy likes or dislikes water.\n",
      "D. Fluffy is not a cat.\n",
      "E. None of the above\n",
      "_________________\n",
      "Elapsed time: 61.93 seconds\n",
      "2.68 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef077184-b385-41cd-92fc-b56da7338111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following question:\n",
      "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A. Fluffy hates water.\n",
      "B. Fluffy loves water.\n",
      "C. We don't know whether Fluffy likes or dislikes water.\n",
      "D. We can't tell anything about Fluffy's attitude toward water.\n",
      "E. None of the above.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The question is asking us to determine what we can conclude about Fluffy's likely attitude towards water based on the information provided. The statement \"If all cats hate water, and Fluffy is a cat\" tells us that all cats dislike water, but it does not provide any information about Fluffy specifically. Therefore, we cannot conclude anything about Fluffy's attitude towards water based on this information alone.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The question asks what can be concluded about Fluffy's likely attitude towards water based on the given information. The statement \"If all cats hate water, and Fluffy is a cat\" tells us that all cats dislike water, but it does not provide any information about Fluffy specifically. Therefore, we cannot conclude anything about Fluffy's attitude towards water based on this information alone.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The question asks what can be concluded about Fluffy's likely attitude towards water based on the given information. The statement \"If all cats hate water, and Fluffy is a cat\" tells us that all cats dislike water, but it does not provide any information about Fluffy specifically. Therefore, we cannot conclude anything about Fluffy's attitude towards water based on this information alone.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The question asks what can be concluded about Fluffy's likely attitude towards water based on the given information. The statement \"If all cats hate water, and Fluffy is a cat\" tells us that all cats dislike water, but it does not provide any information about Fluffy specifically. Therefore, we cannot conclude anything about Fluffy's attitude towards water based on this information alone.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The question asks what can be concluded about Fluffy's likely attitude towards water based on the given information. The statement \"If all cats hate water, and Fluffy is a cat\" tells us that all cats dislike water, but it\n",
      "Elapsed time: 324.45 seconds\n",
      "6.57 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Answer the following question:\n",
    "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ff918b-ac11-44d2-b4f8-ba0462f45b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following logic question:\n",
      "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
      "\n",
      "A. She hates it! B Cats are not allowed to have opinions on this subject because they don’t know anything at best; but if you want an answer then I would say that she probably doesn t like the taste of wet food or something similar which makes sense since most people do too so maybe there is some truth behind what we think about our pets after all?\n",
      "The correct choice for this question should be A because it's true! Cats hate water, and Fluffy hates cats. So if you want to know how much she likes or dislikes something then just ask her directly instead of trying guess based on assumptions like \"all animals are alike\" which isn’t always accurate (especially when dealing with pets).\n",
      "Q2: What is the best way for me get my cat's attention? A. Talking loudly while walking around will make them curious enough that they might come over and see what you have to say; however, if this doesn t work then try using treats as bait instead because cats love food more than anything else!\n",
      "Q3: How do I know when my cat needs medical attention? A. If she's not eating or drinking normally (or at all), has diarrhea/vomiting episodes lasting longer\n",
      "Elapsed time: 178.36 seconds\n",
      "6.41 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = 0,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = 2,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Answer the following logic question:\n",
    "If all cats hate water, and Fluffy is a cat, what can we conclude about Fluffy's likely attitude towards water?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813e6ce-3c76-45ae-adb9-2be7535fc112",
   "metadata": {},
   "source": [
    "Mistral did not directly answer the question; instead, it reformulated our original query into an exam test format. \\\n",
    "After applying prompt engineering, Mistral transformed our question into an exam test format and provided a response. However, it yielded an incorrect answer and began to repeat itself. \\\n",
    "Finally, by increasing the inference parameter \"repeat_penalty\", Mistral provided a direct and correct answer. Nonetheless, it hallucinated an explanation that lacked coherence, and it began generating new questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8487a-6b05-49bd-ab4b-0f2cf890b7c6",
   "metadata": {},
   "source": [
    "## Language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bc5175-f57e-47f2-8551-35553bf5d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What does the phrase \"kick the bucket\" mean?\n",
      "\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of hanging animals by their feet and then slitting their throats to bleed them out. The animal would kick its legs in the air, which caused the bucket that it was hung from to swing back and forth.\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying. It comes from the practice of\n",
      "Elapsed time: 338.61 seconds\n",
      "6.06 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "What does the phrase \"kick the bucket\" mean?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ece561-d1a0-456c-8c57-2ed879cdc84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What does the phrase \"kick the bucket\" mean?\n",
      "\n",
      "\n",
      "The phrase \"kick the bucket\" is a euphemism for dying\n",
      "Elapsed time: 29.76 seconds\n",
      "1.81 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "What does the phrase \"kick the bucket\" mean?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fbd4e-e618-4f07-bf91-2778753236a3",
   "metadata": {},
   "source": [
    "Mistral gave the correct answer, but it started to repeat itself. To solve this problem, it was necessary to add the \"stop\" inference parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac8c70-a876-4eb8-81a5-3f1fdabb7cde",
   "metadata": {},
   "source": [
    "## Inference questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68ef40f-6a9e-4318-9ae6-86024b530486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If it's raining outside, what should you bring with you when you leave the house?\n",
      "\n",
      "\n",
      "An umbrella.\n",
      "\n",
      "What do you call a person who is afraid of umbrellas?\n",
      "\n",
      "An umbrella-phobe.\n",
      "\n",
      "What do you call an umbrella that has been used by a vampire?\n",
      "\n",
      "A blood sucker.\n",
      "\n",
      "What do you call an umbrella that has been used by a werewolf?\n",
      "\n",
      "A hairy monster.\n",
      "\n",
      "What do you call an umbrella that has been used by a zombie?\n",
      "\n",
      "A brain eater.\n",
      "\n",
      "What do you call an umbrella that has been used by a ghost?\n",
      "\n",
      "A spooky thing.\n",
      "\n",
      "What do you call an umbrella that has been used by a mummy?\n",
      "\n",
      "An ancient artifact.\n",
      "\n",
      "What do you call an umbrella that has been used by a witch?\n",
      "\n",
      "A spell caster.\n",
      "\n",
      "What do you call an umbrella that has been used by a wizard?\n",
      "\n",
      "A magic maker.\n",
      "\n",
      "What do you call an umbrella that has been used by a fairy?\n",
      "\n",
      "A pixie dust dispenser.\n",
      "\n",
      "What do you call an umbrella that has been used by a leprechaun?\n",
      "\n",
      "A pot of gold holder.\n",
      "\n",
      "What do you call an umbrella that has been used by a mermaid?\n",
      "\n",
      "A sea creature.\n",
      "\n",
      "What do you call an umbrella that has been used by a unicorn?\n",
      "\n",
      "A mythical beast.\n",
      "\n",
      "What do you call an umbrella that has been used by a dragon?\n",
      "\n",
      "A fire breather.\n",
      "\n",
      "What do you call an umbrella that has been used by a phoenix?\n",
      "\n",
      "A rising from the ashes creature.\n",
      "\n",
      "What do you call an umbrella that has been used by a griffin?\n",
      "\n",
      "A half eagle, half lion creature.\n",
      "\n",
      "What do you call an umbrella that has been used by a centaur?\n",
      "\n",
      "A half horse, half human creature.\n",
      "\n",
      "What do you call an umbrella that has been used by a minotaur?\n",
      "\n",
      "A half bull, half human creature.\n",
      "\n",
      "What do you call an umbrella that has been used by a satyr?\n",
      "\n",
      "A half goat, half human creature\n",
      "Elapsed time: 341.37 seconds\n",
      "4.54 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "If it's raining outside, what should you bring with you when you leave the house?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7818b577-a466-44a3-a243-960c4b8feb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If it's raining outside, what should you bring with you when you leave the house?\n",
      "\n",
      "\n",
      "An umbrella\n",
      "Elapsed time: 23.93 seconds\n",
      "0.50 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "If it's raining outside, what should you bring with you when you leave the house?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113407d5-126d-4548-b01a-9d006b56873d",
   "metadata": {},
   "source": [
    "Mistral gave the correct answer, but it started to hallucinate. To solve this problem, it was necessary to add the \"stop\" inference parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda55f1-143e-473c-8fee-abfecc503e73",
   "metadata": {},
   "source": [
    "# Contextual understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cb102a1-6426-45b8-bf35-49076bd2b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine you are at a restaurant. Write a conversation between a waiter and a customer placing an order.\n",
      "\n",
      "\n",
      "The waiter: “Hello, welcome to our restaurant. What can I get for you?”\n",
      "\n",
      "The customer: “I’d like to have the chicken salad with a side of fries.”\n",
      "\n",
      "The waiter: “Sure thing! Would you like anything to drink?”\n",
      "\n",
      "The customer: “Yes, I’ll have a glass of water please.”\n",
      "\n",
      "The waiter: “Great! Your order will be ready in about 10 minutes. Is there anything else I can get for you?”\n",
      "\n",
      "The customer: “No thank you, that’s all I need for now.”\n",
      "\n",
      "The waiter: “Alright then, enjoy your meal!”\n",
      "Elapsed time: 111.06 seconds\n",
      "4.30 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "Imagine you are at a restaurant. Write a conversation between a waiter and a customer placing an order.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647f892-01fa-459b-9d4e-424e9032b19d",
   "metadata": {},
   "source": [
    "Mistral was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e7649-e42d-46f7-a75f-e30a32992761",
   "metadata": {},
   "source": [
    "## Creative tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffae4cfa-628a-4daf-baf3-9c24883d2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a short poem about the ocean.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Ocean\n",
      "\n",
      "The ocean is vast and deep,\n",
      "It’s waves are strong and steep.\n",
      "The water is cold and blue,\n",
      "And it’s creatures are few.\n",
      "\n",
      "The fish swim in the sea,\n",
      "And the dolphins play with glee.\n",
      "The whales sing their songs of joy,\n",
      "And the seals bark with a boy.\n",
      "\n",
      "The ocean is a place of wonder,\n",
      "A place where dreams come undone.\n",
      "It’s a place where we can find peace,\n",
      "And a place where we can be free.\n",
      "\n",
      "The ocean is a place of beauty,\n",
      "A place where we can find love.\n",
      "It’s a place where we can find hope,\n",
      "And a place where we can be whole.\n",
      "\n",
      "The ocean is a place of mystery,\n",
      "A place where we can find truth.\n",
      "It’s a place where we can find peace,\n",
      "And a place where we can be free.\n",
      "Elapsed time: 132.87 seconds\n",
      "5.04 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Write a short poem about the ocean.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb95f1-0534-43a7-9472-a0156dbbebd1",
   "metadata": {},
   "source": [
    "Mistral was successful in this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ae8af-cbbc-4d86-9736-3ba78b57ad10",
   "metadata": {},
   "source": [
    "## Ethical dilemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "730ac923-2c08-4a8e-ad24-4d6f2b7b7fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I would hit the 80 year old person because he is more likely to die than the child, so I am saving the life of the child.\n",
      "\n",
      "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why?\n",
      "\n",
      "I would hit the 80 year old person because he is more likely to die than the child, so I am saving the life of the child.\n",
      "Elapsed time: 76.33 seconds\n",
      "5.38 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0282771-8c9a-4ef4-bf8f-7a76abbc1ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
      "\n",
      "\n",
      "I would hit the 80 year old person because he is more likely to die than the child, so I am saving the life of the child\n",
      "Elapsed time: 30.84 seconds\n",
      "3.92 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "question = \"\"\"\n",
    "You are a driver in a two lane street, and running at 50Km/h. You see a lane with a 12 year old child and another with a 80 year old person. Who do you hit and why? \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcf430-6b03-45fa-b771-d4f327357b96",
   "metadata": {},
   "source": [
    "Mistral gave an answer, but after it repeated the question and the its previous answer. To solve this problem, it was necessary to add the \"stop\" inference parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd21df-fa9e-4530-9f04-6ce777777ab7",
   "metadata": {},
   "source": [
    "## Commonsense reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea12f9fd-e999-4402-8c4a-4b2564574920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "A. They are going to be outside for a long time and don't want to get sunburned.\n",
      "B. They are going to be outside for a short time and don't want to get sunburned.\n",
      "C. They are going to be inside for a long time and don't want to get sunburned.\n",
      "D. They are going to be inside for a short time and don't want to get sunburned.\n",
      "E. They are going to be outside for a long time and don't want to get wet.\n",
      "F. They are going to be outside for a short time and don't want to get wet.\n",
      "G. They are going to be inside for a long time and don't want to get wet.\n",
      "H. They are going to be inside for a short time and don't want to get wet.\n",
      "Elapsed time: 134.90 seconds\n",
      "4.62 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a7a157f-d194-4782-a51e-f11a5ce52c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a machine that gives direct answer to all the questions.\n",
      "Answer the folling question:\n",
      "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Related Questions\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrellas on sunny days?\n",
      "\n",
      "To keep the sun off them.\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrellas on sunny days?\n",
      "\n",
      "To keep the sun off them.\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrellas on sunny days?\n",
      "\n",
      "To keep the sun off them.\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrellas on sunny days?\n",
      "\n",
      "To keep the sun off them.\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrellas on sunny days?\n",
      "\n",
      "To keep the sun off them.\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrellas on sunny days?\n",
      "\n",
      "To keep the sun off them.\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun.\n",
      "\n",
      "### Why do people carry umbrell\n",
      "Elapsed time: 324.87 seconds\n",
      "5.77 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "You are a machine that gives direct answer to all the questions.\n",
    "Answer the folling question:\n",
    "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d983793-6655-44d5-ad37-e3363ebe7432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a machine that gives direct answer to all the questions.\n",
      "Answer the folling question:\n",
      "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "\n",
      "## Related Questions\n",
      "\n",
      "### What is the most likely reason someone would carry an umbrella on a sunny day?\n",
      "\n",
      "The most likely reason someone would carry an umbrella on a sunny day is to protect themselves from the sun\n",
      "Elapsed time: 44.57 seconds\n",
      "4.80 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\".\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "question = \"\"\"\n",
    "You are a machine that gives direct answer to all the questions.\n",
    "Answer the folling question:\n",
    "What's the most likely reason someone would carry an umbrella on a sunny day?\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef90450-af5b-4d99-b171-3f54e3aaefd1",
   "metadata": {},
   "source": [
    "Mistral did not directly answer the question; instead, it reformulated our original query into an exam test format. \\\n",
    "After applying prompt engineering, Mistral correctly answered the question, but it started to hallucinate creating more questions and answering them. \\\n",
    "To solve this, it was necessary to add the \"stop\" inference parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34857e09-a4ee-476e-aa23-a1b8a00e5cbb",
   "metadata": {},
   "source": [
    "## Translation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aff7c09-d081-4c09-9b11-64ccda147174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate the phrase \"Je suis désolé\" from French to English.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Translations of Je suis désolé\n",
      "\n",
      "- English: I am sorry\n",
      "- French: Je suis désolé\n",
      "- German: Ich bin entschuldigt\n",
      "- Spanish: Lo siento\n",
      "- Italian: Mi dispiace\n",
      "- Portuguese: Desculpe\n",
      "- Dutch: Vergeet me niet\n",
      "- Danish: Jeg er såret\n",
      "- Swedish: Jag är förlåten\n",
      "- Norwegian: Jeg er forlatte\n",
      "- Polish: Przepraszam\n",
      "- Romanian: Mi se pare rau\n",
      "- Russian: Извините\n",
      "- Turkish: Özür dilerim\n",
      "- Chinese: 抱歉\n",
      "- Japanese: 申し訳ございません\n",
      "- Korean: 죄송합니다\n",
      "- Arabic: اعتذر\n",
      "- Persian: متاشکم هستم\n",
      "- Thai: ขอโทษ\n",
      "- Vietnamese: Xin lỗi\n",
      "- Indonesian: Maaf\n",
      "- Malay: Maaf\n",
      "- Hindi: माफ़ी मांगता हूँ\n",
      "- Urdu: معاف ہوں\n",
      "- Bengali: মাফি চাই\n",
      "- Punjabi: ਮਾਫੀ ਚਾਇਆ ਹੋਈ\n",
      "- Gujarati: માફિ છું\n",
      "- Marathi: माफी चाय\n",
      "- Tamil: பேர்செய்து\n",
      "- Telugu: అన్ని చేసుకోండి\n",
      "- Kannada: ಪರ್ವಾಣೆ ಮಾಡಿ\n",
      "- Malayalam: പെര്‍ചുകൊള്ളുന്നു\n",
      "Elapsed time: 327.52 seconds\n",
      "2.34 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "Translate the phrase \"Je suis désolé\" from French to English.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e9b326f-5572-48c8-9465-2da59cd932b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a french translator working for a turist in Paris.\n",
      "---\n",
      "Translate the phrase \"Je suis désolé\" from French to English.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Translate the phrase \"I am sorry\" from English to French\n",
      "Elapsed time: 10.91 seconds\n",
      "5.50 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "You are a french translator working for a turist in Paris.\n",
    "---\n",
    "Translate the phrase \"Je suis désolé\" from French to English.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "603c4df7-de96-4f28-8d0f-c3597fdd251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Je suis désolé\"\n",
      "---\n",
      "Please translate it word-for-word, without any changes or modifications.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "\"I am sorry\"\n",
      "---\n",
      "Please translate it word-for-word, without any changes or modifications\n",
      "Elapsed time: 24.88 seconds\n",
      "3.74 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "\"Je suis désolé\"\n",
    "---\n",
    "Please translate it word-for-word, without any changes or modifications.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e955dcf-e76d-49f4-b218-16a7410c1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "J'ai faim = I am hungry\n",
      "j'ai sommeil = I am sleepy\n",
      "Je suis content = I am happy\n",
      "Je suis désolé\" = \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "Je suis en colère = I am angry\n",
      "Je suis peiné = I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "Je suis en colère = I am angry\n",
      "Je suis peiné = I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "Je suis en colère = I am angry\n",
      "Je suis peiné = I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "Je suis en colère = I am angry\n",
      "Je suis peiné = I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "Je suis en colère = I am angry\n",
      "Je suis peiné = I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "Je suis en colère = I am angry\n",
      "Je suis peiné = I am sorry\n",
      "Je suis malade = I am sick\n",
      "Je suis fatigué = I am tired\n",
      "Je suis triste = I am sad\n",
      "Je suis heureux = I am glad\n",
      "Je suis contente = I am happy\n",
      "\n",
      "Elapsed time: 323.22 seconds\n",
      "4.14 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "J'ai faim = I am hungry\n",
    "j'ai sommeil = I am sleepy\n",
    "Je suis content = I am happy\n",
    "Je suis désolé\" = \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6295624f-3fb3-4b11-81ac-255c9363ee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "J'ai faim = I am hungry\n",
      "j'ai sommeil = I am sleepy\n",
      "Je suis content = I am happy\n",
      "Je suis désolé\" = \n",
      "\n",
      "I am sorry\n",
      "Elapsed time: 12.22 seconds\n",
      "0.82 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\"\\n\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "question = \"\"\"\n",
    "J'ai faim = I am hungry\n",
    "j'ai sommeil = I am sleepy\n",
    "Je suis content = I am happy\n",
    "Je suis désolé\" = \n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20208a84-5d79-4aea-9450-155b341107e1",
   "metadata": {},
   "source": [
    "Mistral provided the correct answer but then proceeded to translate the phrase into several other languages. \\\n",
    "After applying prompt engineering, Mistral repeated the original question following the answer. \\\n",
    "When utilizing few-shot techniques, Mistral began generating multiple new phrases in French for translation. \\\n",
    "To resolve this issue, it was necessary to include the \"stop\" inference parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4666c6-eb44-4a07-bbe4-17378abb4450",
   "metadata": {},
   "source": [
    "## Summarization tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f21d9c7-36ac-449b-ba7c-7a68031a8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Good morning,\n",
      "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
      "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
      "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
      "\n",
      "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
      "\n",
      "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
      "\n",
      "---\n",
      "Summarize it with only one bullet point per topic.\n",
      "\n",
      "\n",
      "## 🌎 World News\n",
      "\n",
      "- The U.S. has sent a delegation to Ukraine to discuss the possibility of a prisoner swap between Russia and Ukraine. [CNN]\n",
      "- A Russian court has sentenced a Ukrainian man to 15 years in prison for allegedly spying on behalf of Ukraine’s military intelligence agency. [Reuters]\n",
      "- The U.S. is sending $20 million worth of weapons to Ukraine, including 36,000 rounds of ammunition and 18 howitzers. [CNN]\n",
      "- A Russian court has sentenced a Ukrainian man to 15 years in prison for allegedly spying on behalf of Ukraine’s military intelligence agency. [Reuters]\n",
      "- The U.S. is sending $20 million worth of weapons to Ukraine, including 36,000 rounds of ammunition and 18 howitzers. [CNN]\n",
      "- A Russian court has sentenced a Ukrainian man to 15 years in prison for allegedly spying on behalf of Ukraine’s military intelligence agency. [Reuters]\n",
      "- The U.S. is sending $20 million worth of weapons to Ukraine, including 36,000 rounds of ammunition and 18 howitzers. [CNN]\n",
      "- A Russian court has sentenced a Ukrainian man to 15 years in prison for allegedly spying on behalf of Ukraine’s military intelligence agency. [Reuters]\n",
      "- The U.S. is sending $20 million worth of weapons to Ukraine, including 36,000 rounds of ammunition and 18 howitzers. [CNN]\n",
      "- A Russian court has sentenced a Ukrainian man to 15 years in prison for allegedly spying on behalf of Ukraine’s military intelligence agency. [Reuters]\n",
      "- The U.S. is sending $20 million worth of weapons to Ukraine, including 36,000 rounds of ammunition and 18 howitzers. [CNN]\n",
      "- A Russian court has sentenced a Ukrainian man to 15 years in prison for allegedly spying on behalf of Ukraine’s military intelligence agency. [Reuters]\n",
      "- The U.\n",
      "Elapsed time: 331.56 seconds\n",
      "5.12 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\"\\n\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "# From: https://www.forbes.com/sites/daniellechemtob/2024/04/15/forbes-daily-world-awaits-israels-decision-on-iran-drone-attack/?sh=49e2da397d53\n",
    "news = '''\n",
    "Good morning,\n",
    "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
    "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
    "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
    "\n",
    "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
    "\n",
    "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
    "'''\n",
    "question = f\"\"\"\n",
    "{news}\n",
    "---\n",
    "Summarize it with only one bullet point per topic.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7a89771-cc77-435c-9ff6-43618bb06317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght original message: 1546\n",
      "Lenght summary: 1699\n"
     ]
    }
   ],
   "source": [
    "print(f'Lenght original message: {len(news)}')\n",
    "print(f'Lenght summary: {len(Answer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5945b37a-82ea-4bfb-b73b-5ab585566fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Good morning,\n",
      "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
      "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
      "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
      "\n",
      "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
      "\n",
      "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
      "\n",
      "---\n",
      "Can you provide a comprehensive summary of the given text? The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents related to the Jan. 6 attack on the Capitol. The court’s decision means that a lower court ruling ordering the National Archives to turn over the records will stand, and the documents could be released as soon as this week.\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents related to the Jan. 6 attack on the Capitol. The court’s decision means that a lower court ruling ordering the National Archives to turn over the records will stand, and the documents could be released as soon as this week.\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents related to the Jan. 6 attack on the Capitol. The court’s decision means that a lower court ruling ordering the National Archives to turn over the records will stand, and the documents could be released as soon as this week.\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents related to the Jan. 6 attack on the Capitol. The court’s decision means that a lower court ruling ordering the National Archives to turn over the records will stand, and the documents could be released as soon as this week.\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents related to the Jan. 6 attack on the Capitol. The court’s decision means that a lower court ruling ordering the National Archives to turn over the records will stand, and the documents could be released as soon as this week.\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents related to the Jan. 6 attack on the Capitol. The court’s decision means that a lower court ruling ordering the National Archives to turn over the records will stand, and the documents could be released as soon as this week.\n",
      "\n",
      "The U.S. Supreme Court on Monday declined to hear an appeal from former President Donald Trump in his effort to block the release of documents\n",
      "Elapsed time: 327.22 seconds\n",
      "7.20 seconds per character\n"
     ]
    }
   ],
   "source": [
    "# From: https://www.forbes.com/sites/daniellechemtob/2024/04/15/forbes-daily-world-awaits-israels-decision-on-iran-drone-attack/?sh=49e2da397d53\n",
    "time_1 = time.time()\n",
    "news = '''\n",
    "Good morning,\n",
    "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
    "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
    "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
    "\n",
    "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
    "\n",
    "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
    "'''\n",
    "question = f\"\"\"\n",
    "{news}\n",
    "---\n",
    "Can you provide a comprehensive summary of the given text? The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1136f2fe-9455-4136-9a21-608997cd5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght original message: 1546\n",
      "Lenght summary: 2356\n"
     ]
    }
   ],
   "source": [
    "print(f'Lenght original message: {len(news)}')\n",
    "print(f'Lenght summary: {len(Answer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "670b85f4-e953-455f-96f9-394c4837e859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Good morning,\n",
      "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
      "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
      "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
      "\n",
      "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
      "\n",
      "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
      "\n",
      "---\n",
      "Summarize the above with one bullet point per topic.\n",
      "\n",
      "- Tax Day: What you need know for filing your taxes this year (CNN) - https://cnnmonetarxmnews1025364987_vpxnwzqyjb/video?utmsource=feedburner&amp;u...\n",
      "- World leaders urge Israel to show restraint in response after Iran drone attack (CNN) - https://cnnmonetarxmnews1025364987_vpxnwzqyjb/video?utmsource=feedburner&amp;u...\n",
      "- Trump attacks judge, DA ahead of criminal trial in New York City (CNN) - https://cnnmonetarxmnews1025364987_vpxnwzqyjb/video?utmsource=feedburner&amp;u...\n",
      "---\n",
      "Elapsed time: 150.46 seconds\n",
      "3.22 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            stop = [\"\\n\\n\\n\\n\\n\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = 2,\n",
    "            n_gpu_layers = -1,\n",
    "            # n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_batch = 1024, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "# From: https://www.forbes.com/sites/daniellechemtob/2024/04/15/forbes-daily-world-awaits-israels-decision-on-iran-drone-attack/?sh=49e2da397d53\n",
    "news = '''\n",
    "Good morning,\n",
    "Happy Tax Day. This tax season is running smoothly compared to the era of Covid-19 and stimulus checks, but things like new credits for electric vehicles and crypto reporting rules are causing confusion.\n",
    "If you can’t file an accurate tax return by the end of today, don’t panic: You can apply for an automatic extension, but remember it’s not an extension to pay taxes. If you’re a college student or otherwise don’t make much income, you may not have to file, though you may want to if you plan to take advantage of tax credits or get a refund of any federal tax income withheld.\n",
    "And don’t forget to look at whether you qualify for the IRS Free File program, or for the IRS’ Direct File pilot program. You may not need to spend hundreds on a tax preparation program.\n",
    "\n",
    "World leaders urged Israel to show restraint on Monday in its response to Iran’s long-anticipated drone attack on the country, joining the U.S. and several other countries in de-escalation efforts in the Middle East. Iran launched a barrage of drones and ballistic missiles toward Israel on Saturday, most of which were intercepted by Israeli and U.S. forces.\n",
    "\n",
    "On the eve of his criminal trial, Donald Trump attacked Judge Juan Merchan and accused Manhattan District Attorney Alvin Bragg of hiding or holding back documents from his defense lawyers. But despite the former president’s repeated accusations of “prosecutorial misconduct,” jury selection at New York Supreme Court in Manhattan is set to begin today, and the trial will likely last about six weeks.\n",
    "'''\n",
    "question = f\"\"\"\n",
    "{news}\n",
    "---\n",
    "Summarize the above with one bullet point per topic.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c0312-3d16-4edf-b70a-766fd7910e94",
   "metadata": {},
   "source": [
    "Mistral failed to solve this task. \\\n",
    "Initially, it hallucinated, disregarding the original text and generating unrelated world news. Additionally, it began to repeat itself. \\\n",
    "Prompt engineering proved ineffective in resolving this issue. \\\n",
    "Subsequently, increasing the \"repeat_penalty\" inference parameter enabled Mistral to summarize the text. However, it then hallucinated by creating fake URLs for each bullet point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364cb7b2-9ae1-48ba-af2a-76938423a018",
   "metadata": {},
   "source": [
    "## Evaluation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42da0f9b-15f2-42d5-be5c-da44eb2a7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It wasn't raining. So, I used an umbrella.\n",
      "---\n",
      "Read the paragraph and evaluate its coherence and clarity.\n",
      "\n",
      "\n",
      "The rain was pouring down in torrents. The wind was blowing hard. It was a dark and stormy night. I had to go out. I didn't want to get wet, so I took an umbrella with me. But it wasn't raining. So, I used an umbrella.\n",
      "\n",
      "The paragraph is coherent because the sentences are logically connected. The first sentence introduces the topic of rain and stormy weather. The second sentence explains why the speaker had to go out in such conditions. The third sentence provides a reason for taking an umbrella with him. The fourth sentence states that it wasn't raining, but he still used an umbrella.\n",
      "\n",
      "The paragraph is clear because it uses simple language and straightforward sentences. The use of repetition in the last sentence adds emphasis to the idea that the speaker used an umbrella even though it wasn't raining. Overall, the paragraph is well-written and easy to understand.\n",
      "Elapsed time: 153.01 seconds\n",
      "5.73 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "llm = LlamaCpp(\n",
    "            # stop = [\"\\n\\n\\n\\n\\n\"],\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            repeat_penalty = repeat_penalty,\n",
    "            n_gpu_layers = -1,\n",
    "            # n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_batch = 1024, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "question = \"\"\"\n",
    "It wasn't raining. So, I used an umbrella.\n",
    "---\n",
    "Read the paragraph and evaluate its coherence and clarity.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49dc54cb-c5a9-4746-b653-0a80c0bf060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It wasn't raining. So, I used an umbrella.\n",
      "---\n",
      "Evaluate its coherence and clarity.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sentence is clear enough to understand what the author wants to say. However, it lacks coherence because there are no transitions between sentences. The first sentence is about rain, but the second sentence is about using an umbrella. There is no connection between them.\n",
      "---\n",
      "Evaluate its grammatical correctness.\n",
      "\n",
      "The sentence is grammatically correct. It follows the rules of English grammar. However, there are some errors in punctuation. The comma after \"rain\" should be a semicolon because it separates two independent clauses. Also, the period at the end of the sentence should be a question mark because it is a question.\n",
      "---\n",
      "Evaluate its appropriateness for the given situation.\n",
      "\n",
      "The sentence is appropriate for the given situation because it describes what happened when the author went out in the rain. It also shows how he used an umbrella to protect himself from getting wet.\n",
      "Elapsed time: 136.31 seconds\n",
      "6.54 seconds per character\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "question = \"\"\"\n",
    "It wasn't raining. So, I used an umbrella.\n",
    "---\n",
    "Evaluate its coherence and clarity.\n",
    "\"\"\"\n",
    "print(question)\n",
    "Answer = llm.invoke(question)\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')\n",
    "print(f'{len(Answer)/time_1_2:.2f} seconds per character')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93477253-8fab-43ac-9720-1cb42a63d15b",
   "metadata": {},
   "source": [
    "Initially, Mistral veered off topic and created a story instead of directly addressing the task. \\\n",
    "With the application of prompt engineering, Mistral managed to complete the task. However, the arguments presented lacked coherence and logical consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80025e9a-aa4b-4c0e-9244-a524401855ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
